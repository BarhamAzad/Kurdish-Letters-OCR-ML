{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0b7c49e2",
      "metadata": {},
      "source": [
        "# Kurdish Full Alphabet OCR - Advanced Testing Pipeline\n",
        "\n",
        "## Overview\n",
        "This notebook implements the inference pipeline for the **Advanced Kurdish OCR Model** (ResNet-Style CNN). It is designed to load the high-accuracy model (approx 95.8%) trained in the upgraded training notebook and test it on new data.\n",
        "\n",
        "##  Important Implementation Details\n",
        "Thispipeline requires specific handling:\n",
        "1.  **Architecture Matching:** We must redefine the exact `KurdishAdvancedCNN` class with Projection Layers to match the saved weights.\n",
        "2.  **Preprocessing:** Images must be normalized using `mean=[0.456]` and `std=[0.224]` (matching ImageNet stats) rather than simple division.\n",
        "3.  **Label Recovery:** Class labels are extracted directly from the checkpoint file to ensure 100% alignment with training.\n",
        "\n",
        "## Contents\n",
        "1.  **Setup & Imports:** Configure device (GPU/CPU) and libraries.\n",
        "2.  **Model Definition:** Re-create the Advanced CNN architecture.\n",
        "3.  **Checkpoint Loading:** Smart loader that retrieves weights + label encoders.\n",
        "4.  **Inference Engine:** Prediction function with correct normalization.\n",
        "5.  **Live Testing:** Real-time camera or batch image testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f1ca78a9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from imutils import paths\n",
        "import os\n",
        "\n",
        "# Configuration\n",
        "IMG_SIZE = 64\n",
        "# This must match the filename saved in the training notebook\n",
        "MODEL_PATH = \"kurdish_letter_model_pytorch.pth\" \n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbef9222",
      "metadata": {},
      "source": [
        "## 1. Define Model Architecture\n",
        "**Crucial Step:** We must define the neural network structure **exactly** as it was during training.\n",
        "This includes:\n",
        "* 4 Residual Blocks with Batch Normalization.\n",
        "* **Projection Shortcuts** (1x1 Convs) to handle channel dimension changes (32 $\\to$ 64 $\\to$ 128 $\\to$ 256).\n",
        "* **Adaptive Average Pooling** to handle various input sizes without crashing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "a470f94b",
      "metadata": {},
      "outputs": [],
      "source": [
        "class KurdishAdvancedCNN(nn.Module):\n",
        "    def __init__(self, num_classes, dropout_rate=0.3):\n",
        "        super(KurdishAdvancedCNN, self).__init__()\n",
        "        \n",
        "        # Block 1: Input -> 32\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)\n",
        "        self.dropout1 = nn.Dropout2d(dropout_rate)\n",
        "        \n",
        "        # Block 2: 32 -> 64 (With Projection)\n",
        "        self.project2 = nn.Sequential(nn.Conv2d(32, 64, 1, bias=False), nn.BatchNorm2d(64))\n",
        "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(64)\n",
        "        self.conv4 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(64)\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)\n",
        "        self.dropout2 = nn.Dropout2d(dropout_rate)\n",
        "        \n",
        "        # Block 3: 64 -> 128 (With Projection)\n",
        "        self.project3 = nn.Sequential(nn.Conv2d(64, 128, 1, bias=False), nn.BatchNorm2d(128))\n",
        "        self.conv5 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn5 = nn.BatchNorm2d(128)\n",
        "        self.conv6 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
        "        self.bn6 = nn.BatchNorm2d(128)\n",
        "        self.pool3 = nn.MaxPool2d(2, 2)\n",
        "        self.dropout3 = nn.Dropout2d(dropout_rate)\n",
        "        \n",
        "        # Block 4: 128 -> 256 (With Projection)\n",
        "        self.project4 = nn.Sequential(nn.Conv2d(128, 256, 1, bias=False), nn.BatchNorm2d(256))\n",
        "        self.conv7 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.bn7 = nn.BatchNorm2d(256)\n",
        "        self.conv8 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
        "        self.bn8 = nn.BatchNorm2d(256)\n",
        "        self.pool4 = nn.MaxPool2d(2, 2)\n",
        "        self.dropout4 = nn.Dropout2d(dropout_rate)\n",
        "        \n",
        "        # Head\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc1 = nn.Linear(256, 512)\n",
        "        self.bn_fc1 = nn.BatchNorm1d(512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.bn_fc2 = nn.BatchNorm1d(256)\n",
        "        self.fc3 = nn.Linear(256, num_classes)\n",
        "        self.dropout_fc = nn.Dropout(dropout_rate)\n",
        "        self.leaky_relu = nn.LeakyReLU(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.leaky_relu(self.bn1(self.conv1(x)))\n",
        "        res = x\n",
        "        x = self.leaky_relu(self.bn2(self.conv2(x)))\n",
        "        x = self.leaky_relu(x + res)\n",
        "        x = self.pool1(self.dropout1(x))\n",
        "        \n",
        "        res = self.project2(x)\n",
        "        x = self.leaky_relu(self.bn3(self.conv3(x)))\n",
        "        x = self.bn4(self.conv4(x))\n",
        "        x = self.leaky_relu(x + res)\n",
        "        x = self.pool2(self.dropout2(x))\n",
        "        \n",
        "        res = self.project3(x)\n",
        "        x = self.leaky_relu(self.bn5(self.conv5(x)))\n",
        "        x = self.bn6(self.conv6(x))\n",
        "        x = self.leaky_relu(x + res)\n",
        "        x = self.pool3(self.dropout3(x))\n",
        "        \n",
        "        res = self.project4(x)\n",
        "        x = self.leaky_relu(self.bn7(self.conv7(x)))\n",
        "        x = self.bn8(self.conv8(x))\n",
        "        x = self.leaky_relu(x + res)\n",
        "        x = self.pool4(self.dropout4(x))\n",
        "        \n",
        "        x = self.global_avg_pool(x).view(x.size(0), -1)\n",
        "        x = self.leaky_relu(self.bn_fc1(self.fc1(x)))\n",
        "        x = self.dropout_fc(x)\n",
        "        x = self.leaky_relu(self.bn_fc2(self.fc2(x)))\n",
        "        x = self.dropout_fc(x)\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18b70cd7",
      "metadata": {},
      "source": [
        "## 2. Load \"Gold Standard\" Checkpoint\n",
        "This function loads `kurdish_letter_model_pytorch.pth`.\n",
        "* **Automatic Class Detection:** It reads the `label_encoder` saved inside the file, so you don't need a separate `.pkl` file.\n",
        "* **Weights Loading:** It maps the trained weights to the architecture defined above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "926c8cc5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading checkpoint from: kurdish_letter_model_pytorch.pth\n",
            "‚úÖ Found 33 classes in checkpoint.\n",
            "‚úÖ Model loaded successfully! (Trained for 36 epochs)\n",
            "   Validation Accuracy: 95.87%\n"
          ]
        }
      ],
      "source": [
        "def load_trained_model(model_path):\n",
        "    print(f\"Loading checkpoint from: {model_path}\")\n",
        "    \n",
        "    # FIX: Set weights_only=False to allow loading the LabelEncoder object\n",
        "    checkpoint = torch.load(model_path, map_location=device, weights_only=False)\n",
        "    \n",
        "    # 1. Retrieve Label Encoder\n",
        "    if 'label_encoder' in checkpoint:\n",
        "        le = checkpoint['label_encoder']\n",
        "        classes = le.classes_\n",
        "        print(f\"‚úÖ Found {len(classes)} classes in checkpoint.\")\n",
        "    else:\n",
        "        # Fallback if you used an older training loop\n",
        "        raise ValueError(\"Label encoder not found in checkpoint! Please re-train with the upgraded notebook.\")\n",
        "        \n",
        "    # 2. Initialize Model with correct class count\n",
        "    model = KurdishAdvancedCNN(num_classes=len(classes)).to(device)\n",
        "    \n",
        "    # 3. Load Weights\n",
        "    if 'model_state_dict' in checkpoint:\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    else:\n",
        "        model.load_state_dict(checkpoint) # Fallback for older saves\n",
        "        \n",
        "    model.eval() # Set to evaluation mode\n",
        "    print(f\"‚úÖ Model loaded successfully! (Trained for {checkpoint.get('epoch', '?')} epochs)\")\n",
        "    print(f\"   Validation Accuracy: {checkpoint.get('val_accuracy', 0):.2f}%\")\n",
        "    \n",
        "    return model, le\n",
        "\n",
        "# Execute load\n",
        "try:\n",
        "    model, label_encoder = load_trained_model(MODEL_PATH)\n",
        "except FileNotFoundError:\n",
        "    print(f\"‚ùå Error: {MODEL_PATH} not found. Please run training first!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2eb80a25",
      "metadata": {},
      "source": [
        "## 3. Inference & Preprocessing\n",
        "The `predict_image` function handles the transition from a raw image to a model tensor.\n",
        "\n",
        "**Preprocessing Pipeline:**\n",
        "1.  **Resize:** Force image to 64x64.\n",
        "2.  **Channel Expansion:** Convert Grayscale $\\to$ 3-Channel (Model expects 3 inputs).\n",
        "3.  **Normalization:** Apply the specific formula: `(pixel - 0.456) / 0.224`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "805b3e80",
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_image(image_path, model, label_encoder):\n",
        "    # 1. Read Image\n",
        "    if isinstance(image_path, str):\n",
        "        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "    else:\n",
        "        image = image_path # Assume it's already a numpy array (e.g. from camera)\n",
        "        \n",
        "    if image is None:\n",
        "        return \"Error reading image\", 0.0\n",
        "\n",
        "    # 2. Preprocessing (MUST MATCH TRAINING)\n",
        "    # Resize\n",
        "    img = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n",
        "    \n",
        "    # Stack to 3 Channels (Because model expects 3)\n",
        "    img = np.stack([img] * 3, axis=-1)\n",
        "    \n",
        "    # Normalize & Standardize\n",
        "    # (pixel / 255.0 - mean) / std\n",
        "    img = img.astype('float32') / 255.0\n",
        "    img = (img - 0.456) / 0.224\n",
        "    \n",
        "    # Convert to Tensor (CHW format)\n",
        "    img = np.transpose(img, (2, 0, 1))\n",
        "    img_tensor = torch.from_numpy(img).float().unsqueeze(0).to(device)\n",
        "    \n",
        "    # 3. Inference\n",
        "    with torch.no_grad():\n",
        "        outputs = model(img_tensor)\n",
        "        probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
        "        \n",
        "        # Get Top Prediction\n",
        "        prob, predicted_idx = torch.max(probabilities, 1)\n",
        "        confidence = prob.item() * 100\n",
        "        predicted_label = label_encoder.inverse_transform([predicted_idx.item()])[0]\n",
        "        \n",
        "    return predicted_label, confidence, image\n",
        "\n",
        "# Test on a random file (if you have one)\n",
        "# label, conf, _ = predict_image(\"path/to/test.jpg\", model, label_encoder)\n",
        "# print(f\"Prediction: {label} ({conf:.1f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00f28f11",
      "metadata": {},
      "source": [
        "## üé• Live Camera GUI ‚Äì Advanced Kurdish OCR\n",
        "\n",
        "This interactive tool provides a real-time feed from your webcam to test the model in a live environment.\n",
        "\n",
        "### How it Works (Advanced Pipeline)\n",
        "When you click **Capture ROI**, the system performs the following \"Gold Standard\" preprocessing steps to match the training data exactly:\n",
        "1.  **ROI Extraction:** Crops the center square from the video feed.\n",
        "2.  **3-Channel Conversion:** Converts the grayscale input into a 3-channel format (RGB/BGR) as required by the ResNet architecture.\n",
        "3.  **Normalization:** Applies ImageNet statistics (`mean=0.456`, `std=0.224`) to center the pixel data.\n",
        "4.  **Inference:** The processed tensor is fed into the **Advanced CNN**, which outputs the predicted class and confidence score.\n",
        "\n",
        "### Controls\n",
        "* **Camera:** Select your camera input source (Index 0, 1, or 2).\n",
        "* **Start:** Initializes the camera thread and begins the live preview.\n",
        "* **Stop:** Safely releases the camera and stops the thread.\n",
        "* **Capture ROI:** Freezes the current frame, processes the region inside the box, and runs the prediction.\n",
        "\n",
        "### Tips for Best Results\n",
        "* **Contrast:** Use a **black marker on white paper**. The model is trained on high-contrast character data.\n",
        "* **Alignment:** Center the letter inside the **Blue/Orange Box**.\n",
        "* **Lighting:** Ensure the paper is well-lit to avoid shadows, which can be mistaken for strokes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "3f4e8d95",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9ffe2f3e6e4f463bbdbfa8e748de5a29",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value=\"\\n    <div style='font-family:Inter,Helvetica,Arial,sans-serif; padding:8px 0;'>\\n ‚Ä¶"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "## Interactive GUI: Camera ROI Capture and Prediction (robust, with diagnostics)\n",
        "\n",
        "# Enhancements:\n",
        "# - Camera index selector (0/1/2) and resolution hints\n",
        "# - Immediate single-frame test after start to confirm preview\n",
        "# - Heartbeat indicator updated from the preview loop\n",
        "# - Defensive checks and clear status messages\n",
        "# - Stop always releases camera; Start prevents duplicate threads\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import time\n",
        "import threading\n",
        "from IPython.display import display\n",
        "import ipywidgets as widgets\n",
        "import torch\n",
        "\n",
        "# --- BACKEND UPGRADES FOR ADVANCED CNN ---\n",
        "\n",
        "# Ensure variable compatibility (The GUI uses 'le', our loader uses 'label_encoder')\n",
        "le = label_encoder \n",
        "\n",
        "# Helper: predict from a grayscale numpy array (already 2D)\n",
        "def predict_from_array(gray_array):\n",
        "    assert len(gray_array.shape) == 2, \"Expected grayscale 2D array\"\n",
        "    \n",
        "    # 1. Resize to match Model Input (64x64)\n",
        "    img_resized = cv2.resize(gray_array, (IMG_SIZE, IMG_SIZE))\n",
        "    \n",
        "    # 2. Convert to Float and Scale [0, 1]\n",
        "    img_float = img_resized.astype('float32') / 255.0\n",
        "    \n",
        "    # 3. Stack to 3 Channels (Model expects RGB/BGR depth, even if grayscale)\n",
        "    img_rgb = np.stack([img_float] * 3, axis=-1)\n",
        "    \n",
        "    # 4. Normalize using Training Stats (ImageNet Mean/Std)\n",
        "    # This is CRITICAL for the Advanced ResNet model\n",
        "    img_norm = (img_rgb - 0.456) / 0.224\n",
        "    \n",
        "    # 5. Convert to Tensor (CHW format)\n",
        "    # Permute moves channel to first dimension: (64, 64, 3) -> (3, 64, 64)\n",
        "    img_tensor = torch.from_numpy(img_norm).permute(2, 0, 1).unsqueeze(0).float().to(device)\n",
        "    \n",
        "    # 6. Inference\n",
        "    with torch.no_grad():\n",
        "        output = model(img_tensor)\n",
        "        probabilities = torch.softmax(output, dim=1)[0].cpu().numpy()\n",
        "        predicted_idx = int(torch.argmax(output, dim=1).item())\n",
        "        confidence = float(probabilities[predicted_idx] * 100)\n",
        "    \n",
        "    predicted_class = le.classes_[predicted_idx]\n",
        "    return predicted_class, confidence, probabilities\n",
        "\n",
        "# --- GUI DEFINITION (UNCHANGED) ---\n",
        "\n",
        "# Widgets\n",
        "cam_index_dropdown = widgets.Dropdown(options=[0,1,2], value=0, description='Camera', layout=widgets.Layout(width='180px'))\n",
        "start_button = widgets.Button(description=\"Start\", button_style=\"success\")\n",
        "stop_button = widgets.Button(description=\"Stop\", button_style=\"warning\")\n",
        "capture_button = widgets.Button(description=\"Capture ROI\", button_style=\"primary\")\n",
        "status_label = widgets.HTML(value=\"<b>Status:</b> Ready\")\n",
        "heartbeat_label = widgets.HTML(value=\"<b>Heartbeat:</b> idle\")\n",
        "output_area = widgets.Output()\n",
        "preview_area = widgets.VBox([])\n",
        "image_widget = widgets.Image(format='png')\n",
        "preview_area.children = [image_widget]\n",
        "\n",
        "controls = widgets.HBox([cam_index_dropdown, start_button, stop_button, capture_button])\n",
        "ui = widgets.VBox([\n",
        "    widgets.HTML(\"\"\"\n",
        "    <div style='font-family:Inter,Helvetica,Arial,sans-serif; padding:8px 0;'>\n",
        "      <h2 style='margin:0 0 8px;'>Kurdish Full Alphabet OCR ‚Äì Live Capture</h2>\n",
        "      <p style='color:#555; margin:0;'>Place a white paper with a black Kurdish letter within the ROI, then press <b>Capture ROI</b>.</p>\n",
        "    </div>\n",
        "    \"\"\"),\n",
        "    controls,\n",
        "    status_label,\n",
        "    heartbeat_label,\n",
        "    preview_area,\n",
        "    output_area\n",
        "])\n",
        "\n",
        "display(ui)\n",
        "\n",
        "# Camera and ROI config\n",
        "cap = None\n",
        "running = False\n",
        "preview_thread = None\n",
        "last_frame = None\n",
        "roi = None  # (x,y,w,h)\n",
        "COLOR_ROI = (0,200,255)\n",
        "\n",
        "# Frame overlay\n",
        "def draw_overlay(frame):\n",
        "    h, w = frame.shape[:2]\n",
        "    global roi\n",
        "    if roi is None:\n",
        "        rw = int(w * 0.6); rh = int(h * 0.6)\n",
        "        rx = (w - rw) // 2; ry = (h - rh) // 2\n",
        "        roi = (rx, ry, rw, rh)\n",
        "    rx, ry, rw, rh = roi\n",
        "    overlay = frame.copy()\n",
        "    mask = np.zeros_like(frame)\n",
        "    cv2.rectangle(mask, (0,0), (w,h), (0,0,0), -1)\n",
        "    cv2.rectangle(mask, (rx, ry), (rx+rw, ry+rh), (0,0,0), -1)\n",
        "    alpha = 0.35\n",
        "    cv2.addWeighted(mask, alpha, overlay, 1-alpha, 0, overlay)\n",
        "    frame = overlay\n",
        "    cv2.rectangle(frame, (rx, ry), (rx+rw, ry+rh), COLOR_ROI, 2)\n",
        "    cv2.putText(frame, 'Align paper within ROI', (rx+10, max(30, ry-10)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, COLOR_ROI, 2, cv2.LINE_AA)\n",
        "    return frame\n",
        "\n",
        "# Binarize to white background / black text\n",
        "def to_binary(gray):\n",
        "    blur = cv2.GaussianBlur(gray, (5,5), 0)\n",
        "    _, thresh = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
        "    if np.mean(thresh) < 127:\n",
        "        thresh = 255 - thresh\n",
        "    return thresh\n",
        "\n",
        "# Preview loop\n",
        "def preview_loop():\n",
        "    global cap, running, last_frame\n",
        "    frame_count = 0\n",
        "    while running and cap is not None and cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            status_label.value = \"<b>Status:</b> Camera read failed\"\n",
        "            break\n",
        "        target_w = 800\n",
        "        target_h = int(frame.shape[0] * target_w / frame.shape[1])\n",
        "        frame = cv2.resize(frame, (target_w, target_h))\n",
        "        last_frame = frame.copy()\n",
        "        frame_disp = draw_overlay(frame.copy())\n",
        "        rgb = cv2.cvtColor(frame_disp, cv2.COLOR_BGR2RGB)\n",
        "        ok, buf = cv2.imencode('.png', rgb)\n",
        "        if ok:\n",
        "            image_widget.value = buf.tobytes()\n",
        "        frame_count += 1\n",
        "        if frame_count % 15 == 0:\n",
        "            heartbeat_label.value = f\"<b>Heartbeat:</b> {frame_count} frames\"\n",
        "        time.sleep(0.02)\n",
        "    heartbeat_label.value = \"<b>Heartbeat:</b> stopped\"\n",
        "\n",
        "# Event handlers\n",
        "def on_start_clicked(_):\n",
        "    global cap, running, preview_thread, last_frame\n",
        "    if running:\n",
        "        status_label.value = \"<b>Status:</b> Preview already running\"\n",
        "        return\n",
        "    cam_idx = cam_index_dropdown.value\n",
        "    cap = cv2.VideoCapture(cam_idx)\n",
        "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
        "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
        "    if not cap.isOpened():\n",
        "        status_label.value = f\"<b>Status:</b> Could not open camera index {cam_idx}\"\n",
        "        cap = None\n",
        "        return\n",
        "    ret, frame = cap.read()\n",
        "    if not ret or frame is None:\n",
        "        status_label.value = \"<b>Status:</b> Camera delivered no frames\"\n",
        "        cap.release(); cap = None\n",
        "        return\n",
        "    target_w = 800\n",
        "    target_h = int(frame.shape[0] * target_w / frame.shape[1])\n",
        "    frame = cv2.resize(frame, (target_w, target_h))\n",
        "    last_frame = frame.copy()\n",
        "    frame_disp = draw_overlay(frame.copy())\n",
        "    rgb = cv2.cvtColor(frame_disp, cv2.COLOR_BGR2RGB)\n",
        "    ok, buf = cv2.imencode('.png', rgb)\n",
        "    if ok:\n",
        "        image_widget.value = buf.tobytes()\n",
        "    status_label.value = \"<b>Status:</b> Camera started\"\n",
        "    heartbeat_label.value = \"<b>Heartbeat:</b> starting\"\n",
        "    running = True\n",
        "    preview_thread = threading.Thread(target=preview_loop)\n",
        "    preview_thread.daemon = True\n",
        "    preview_thread.start()\n",
        "\n",
        "def on_stop_clicked(_):\n",
        "    global cap, running, preview_thread\n",
        "    running = False\n",
        "    if cap:\n",
        "        cap.release()\n",
        "        cap = None\n",
        "    status_label.value = \"<b>Status:</b> Camera stopped\"\n",
        "    image_widget.value = b''  # Clear image\n",
        "\n",
        "def on_capture_clicked(_):\n",
        "    if not last_frame is not None:\n",
        "        status_label.value = \"<b>Status:</b> No frame to capture\"\n",
        "        return\n",
        "    \n",
        "    if roi is None:\n",
        "        status_label.value = \"<b>Status:</b> ROI not defined\"\n",
        "        return\n",
        "    \n",
        "    rx, ry, rw, rh = roi\n",
        "    roi_img = last_frame[ry:ry+rh, rx:rx+rw]\n",
        "    gray_roi = cv2.cvtColor(roi_img, cv2.COLOR_BGR2GRAY)\n",
        "    \n",
        "    # Display ROI and its binary version side by side\n",
        "    binary = to_binary(gray_roi)\n",
        "    combined = np.hstack([cv2.cvtColor(gray_roi, cv2.COLOR_GRAY2BGR), cv2.cvtColor(binary, cv2.COLOR_GRAY2BGR)])\n",
        "    \n",
        "    # Predict\n",
        "    pred_class, confidence, probs = predict_from_array(gray_roi)\n",
        "    \n",
        "    # Show results\n",
        "    with output_area:\n",
        "        output_area.clear_output(wait=True)\n",
        "        \n",
        "        # Convert to RGB for display in Jupyter\n",
        "        combined_rgb = cv2.cvtColor(combined, cv2.COLOR_BGR2RGB)\n",
        "        _, buf = cv2.imencode('.png', combined_rgb)\n",
        "        \n",
        "        print(f\"Predicted: {pred_class}\")\n",
        "        print(f\"Confidence: {confidence:.2f}%\")\n",
        "        \n",
        "        # Show top 5 predictions\n",
        "        top_5_idx = np.argsort(probs)[::-1][:5]\n",
        "        print(\"Top 5 predictions:\")\n",
        "        for idx in top_5_idx:\n",
        "            class_name = le.classes_[idx]\n",
        "            prob = probs[idx] * 100\n",
        "            print(f\"  {class_name}: {prob:.2f}%\")\n",
        "\n",
        "start_button.on_click(on_start_clicked)\n",
        "stop_button.on_click(on_stop_clicked)\n",
        "capture_button.on_click(on_capture_clicked)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "c8c31448",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def evaluate_model_comprehensive(model, loader, device, label_encoder, top_k=3):\n",
        "    \"\"\"\n",
        "    Runs a full evaluation suite: Confusion Matrix, Classification Report, \n",
        "    Top-K Accuracy, and Error Analysis.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_probs = []\n",
        "    \n",
        "    # Store images for error visualization\n",
        "    error_images = []\n",
        "    error_true = []\n",
        "    error_pred = []\n",
        "    error_conf = []\n",
        "    \n",
        "    print(f\"üìä Running Comprehensive Evaluation on {len(loader.dataset)} images...\")\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            probs = torch.softmax(outputs, dim=1)\n",
        "            \n",
        "            # Get Top-1 prediction\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            \n",
        "            # Store data\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_probs.extend(probs.cpu().numpy())\n",
        "            \n",
        "            # Capture Errors\n",
        "            incorrect_mask = preds != labels\n",
        "            if incorrect_mask.any():\n",
        "                bad_imgs = inputs[incorrect_mask].cpu()\n",
        "                bad_preds = preds[incorrect_mask].cpu()\n",
        "                bad_true = labels[incorrect_mask].cpu()\n",
        "                bad_conf = torch.max(probs, dim=1)[0][incorrect_mask].cpu()\n",
        "                \n",
        "                for i in range(len(bad_imgs)):\n",
        "                    if len(error_images) < 20: # Limit to saving 20 errors\n",
        "                        error_images.append(bad_imgs[i])\n",
        "                        error_true.append(bad_true[i].item())\n",
        "                        error_pred.append(bad_preds[i].item())\n",
        "                        error_conf.append(bad_conf[i].item())\n",
        "\n",
        "    # --- METRIC 1: Overall Accuracy ---\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    print(f\"\\n‚úÖ Top-1 Accuracy: {acc*100:.2f}%\")\n",
        "    \n",
        "    # --- METRIC 2: Top-K Accuracy ---\n",
        "    all_labels = np.array(all_labels)\n",
        "    all_probs = np.array(all_probs)\n",
        "    top_k_hits = 0\n",
        "    for i in range(len(all_labels)):\n",
        "        # Get indices of top K probabilities\n",
        "        top_indices = all_probs[i].argsort()[-top_k:][::-1]\n",
        "        if all_labels[i] in top_indices:\n",
        "            top_k_hits += 1\n",
        "            \n",
        "    top_k_acc = top_k_hits / len(all_labels)\n",
        "    print(f\"‚úÖ Top-{top_k} Accuracy: {top_k_acc*100:.2f}%\")\n",
        "    \n",
        "    # --- METRIC 3: Classification Report ---\n",
        "    class_names = label_encoder.classes_\n",
        "    print(f\"\\nüìù Classification Report (Per-Class Performance):\")\n",
        "    print(classification_report(all_labels, all_preds, target_names=class_names, digits=4))\n",
        "    \n",
        "    # --- PLOT 1: Confusion Matrix Heatmap ---\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    plt.figure(figsize=(20, 16))\n",
        "    sns.heatmap(cm, annot=False, fmt='d', cmap='Blues', \n",
        "                xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.title('Confusion Matrix Heatmap')\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # --- PLOT 2: Top Confused Pairs ---\n",
        "    # Find which pairs are most frequently confused\n",
        "    np.fill_diagonal(cm, 0) # Ignore correct predictions\n",
        "    pairs = []\n",
        "    for i in range(len(class_names)):\n",
        "        for j in range(len(class_names)):\n",
        "            if cm[i, j] > 0:\n",
        "                pairs.append((class_names[i], class_names[j], cm[i, j]))\n",
        "    \n",
        "    # Sort by count\n",
        "    pairs.sort(key=lambda x: x[2], reverse=True)\n",
        "    \n",
        "    print(\"\\n‚ö†Ô∏è Most Common Confusions (True -> Predicted):\")\n",
        "    for true_l, pred_l, count in pairs[:10]:\n",
        "        print(f\"   ‚Ä¢ '{true_l}'  mistaken for  '{pred_l}' : {count} times\")\n",
        "        \n",
        "    # --- PLOT 3: Visual Error Gallery ---\n",
        "    if len(error_images) > 0:\n",
        "        print(\"\\nüñºÔ∏è Gallery of Mistakes (High Confidence Errors):\")\n",
        "        cols = 5\n",
        "        rows = min(4, (len(error_images) + cols - 1) // cols)\n",
        "        plt.figure(figsize=(15, 3*rows))\n",
        "        \n",
        "        for i in range(min(len(error_images), 20)):\n",
        "            plt.subplot(rows, cols, i+1)\n",
        "            # Un-normalize for display: (img * std) + mean\n",
        "            img = error_images[i].permute(1, 2, 0).numpy()\n",
        "            img = (img * 0.224) + 0.456\n",
        "            img = np.clip(img, 0, 1)\n",
        "            \n",
        "            true_name = label_encoder.inverse_transform([error_true[i]])[0]\n",
        "            pred_name = label_encoder.inverse_transform([error_pred[i]])[0]\n",
        "            \n",
        "            plt.imshow(img)\n",
        "            plt.title(f\"True: {true_name}\\nPred: {pred_name}\\nConf: {error_conf[i]*100:.0f}%\", color='red')\n",
        "            plt.axis('off')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# --- USAGE ---\n",
        "# evaluate_model_comprehensive(model, val_loader, device, kurdish_dataset.label_encoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "f1ef349d",
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'val_loader' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m evaluate_model_comprehensive(model, \u001b[43mval_loader\u001b[49m, device, kurdish_dataset.label_encoder)\n",
            "\u001b[31mNameError\u001b[39m: name 'val_loader' is not defined"
          ]
        }
      ],
      "source": [
        "evaluate_model_comprehensive(model, val_loader, device, kurdish_dataset.label_encoder)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python3.12_ML_venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
