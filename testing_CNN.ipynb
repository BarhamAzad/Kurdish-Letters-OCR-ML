{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b7c49e2",
   "metadata": {},
   "source": [
    "# Kurdish Letter OCR - Model Testing Notebook\n",
    "\n",
    "## Overview\n",
    "This notebook loads the trained Kurdish letter OCR model and tests it on new images.\n",
    "\n",
    "## Contents\n",
    "1. **Setup** - Load model and dependencies\n",
    "2. **Prediction Function** - Single image classification\n",
    "3. **Batch Testing** - Test multiple images at once\n",
    "4. **Visualization** - Display results with confidence scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01381e6e",
   "metadata": {},
   "source": [
    "## About this notebook\n",
    "\n",
    "This notebook demonstrates inference for a trained Kurdish Letter OCR model and provides a live camera GUI to capture a Region of Interest (ROI), binarize it, and classify it.\n",
    "\n",
    "What you can do here:\n",
    "- Load the model and verify the environment\n",
    "- Predict single images from disk\n",
    "- Use a threaded live camera preview with ROI, capture, and classification\n",
    "\n",
    "Quick start:\n",
    "1) Run cells from top to bottom until the \"Interactive GUI\" section\n",
    "2) In the GUI, click Start, align the letter in the highlighted ROI, then click Capture ROI\n",
    "3) The predicted Kurdish letter (large), and its confidence + per-class probabilities will appear\n",
    "\n",
    "Notes:\n",
    "- Input size is 64×64 grayscale\n",
    "- Only the predicted letter is rendered large; other labels (Predicted, Confidence) are normal-sized\n",
    "- Class display uses Kurdish letters mapped from training folders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f1ca78a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import glob\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbef9222",
   "metadata": {},
   "source": [
    "## Configuration & Model Definition\n",
    "\n",
    "Set up parameters and define the CNN model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a470f94b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Configuration parameters (must match training parameters)\n",
    "IMG_SIZE = 64\n",
    "CHANNELS = 1\n",
    "MODEL_PATH = \"kurdish_letter_model_pytorch.pth\"\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Categories (must match training)\n",
    "CATEGORIES = [\n",
    "    {\"folder\": \"EEE_letters\", \"prefix\": \"eee_letter_\"},\n",
    "    {\"folder\": \"LLL_letters\", \"prefix\": \"lll_letter_\"},\n",
    "    {\"folder\": \"OOO_letters\", \"prefix\": \"ooo_letter_\"},\n",
    "    {\"folder\": \"RRR_letters\", \"prefix\": \"rrr_letter_\"},\n",
    "    {\"folder\": \"VVV_letters\", \"prefix\": \"vvv_letter_\"}\n",
    "]\n",
    "\n",
    "# Display mapping from folder/class names to Kurdish letters\n",
    "CLASS_DISPLAY_MAP = {\n",
    "    \"EEE_letters\": \"ێ\",\n",
    "    \"LLL_letters\": \"ڵ\",\n",
    "    \"OOO_letters\": \"ۆ\",\n",
    "    \"RRR_letters\": \"ڕ\",\n",
    "    \"VVV_letters\": \"ڤ\",\n",
    "}\n",
    "\n",
    "EXTENSIONS = [\".png\", \".jpg\", \".jpeg\", \".bmp\"]\n",
    "\n",
    "class KurdishCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional Neural Network for Kurdish letter classification.\n",
    "    \n",
    "    Architecture:\n",
    "    - 3 convolutional layers with ReLU activation\n",
    "    - MaxPooling after each convolution\n",
    "    - 2 fully connected layers with dropout\n",
    "    - Output: 5 classes (Kurdish letters)\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes):\n",
    "        super(KurdishCNN, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # Calculate flattened features\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, 1, IMG_SIZE, IMG_SIZE)\n",
    "            x = self.pool(self.relu(self.conv1(dummy_input)))\n",
    "            x = self.pool(self.relu(self.conv2(x)))\n",
    "            x = self.pool(self.relu(self.conv3(x)))\n",
    "            num_flat_features = x.numel() // x.shape[0]\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(num_flat_features, 64)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = self.pool(self.relu(self.conv3(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b70cd7",
   "metadata": {},
   "source": [
    "## Model Loading\n",
    "\n",
    "Load the trained model weights and prepare for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "926c8cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes available: ['EEE_letters' 'LLL_letters' 'OOO_letters' 'RRR_letters' 'VVV_letters']\n",
      "\n",
      "✓ Model loaded from: kurdish_letter_model_pytorch.pth\n",
      "✓ Model set to evaluation mode\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize label encoder with training categories\n",
    "le = LabelEncoder()\n",
    "le.fit(np.array([cat[\"folder\"] for cat in CATEGORIES]))\n",
    "\n",
    "print(f\"Classes available: {le.classes_}\\n\")\n",
    "\n",
    "# Load the model\n",
    "model = KurdishCNN(num_classes=len(CATEGORIES)).to(device)\n",
    "\n",
    "# Load saved weights\n",
    "if os.path.exists(MODEL_PATH):\n",
    "    model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "    print(f\"✓ Model loaded from: {MODEL_PATH}\")\n",
    "else:\n",
    "    print(f\"✗ Model file not found: {MODEL_PATH}\")\n",
    "    print(\"Please train the model first using Kurdish_Letter_OCR_CNN.ipynb\")\n",
    "\n",
    "# Set model to evaluation mode (disables dropout, batch norm, etc.)\n",
    "model.eval()\n",
    "print(\"✓ Model set to evaluation mode\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb80a25",
   "metadata": {},
   "source": [
    "## Prediction Functions\n",
    "\n",
    "Functions to predict class for single or batch images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "805b3e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single_image(image_path):\n",
    "    \"\"\"\n",
    "    Predict the class of a single image.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    image_path : str\n",
    "        Path to the image file\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (predicted_class_display, confidence, probabilities)\n",
    "        - predicted_class_display: str - The predicted Kurdish letter (mapped)\n",
    "        - confidence: float - Confidence score (0-100)\n",
    "        - probabilities: np.array - Confidence for each class\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read image in grayscale\n",
    "        img_array = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        if img_array is None:\n",
    "            print(f\"✗ Error: Could not load image from {image_path}\")\n",
    "            return None, None, None\n",
    "        \n",
    "        # Resize to model input size\n",
    "        img_resized = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))\n",
    "        \n",
    "        # Normalize (0-1 range)\n",
    "        img_normalized = img_resized / 255.0\n",
    "        \n",
    "        # Convert to tensor (batch_size=1, channels=1)\n",
    "        img_tensor = torch.tensor(img_normalized, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        # Move to device\n",
    "        img_tensor = img_tensor.to(device)\n",
    "        \n",
    "        # Forward pass (no gradient computation)\n",
    "        with torch.no_grad():\n",
    "            output = model(img_tensor)\n",
    "            probabilities = torch.softmax(output, dim=1)[0].cpu().numpy()\n",
    "            predicted_idx = torch.argmax(output, dim=1).item()\n",
    "            confidence = probabilities[predicted_idx] * 100\n",
    "        \n",
    "        # Get class name and map to display\n",
    "        raw_class = le.classes_[predicted_idx]\n",
    "        predicted_class_display = CLASS_DISPLAY_MAP.get(raw_class, raw_class)\n",
    "        \n",
    "        return predicted_class_display, confidence, probabilities\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error processing image: {str(e)}\")\n",
    "        return None, None, None\n",
    "\n",
    "\n",
    "def display_prediction(image_path, predicted_class, confidence, probabilities):\n",
    "    \"\"\"\n",
    "    Display prediction results in a formatted table.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    image_path : str\n",
    "        Path to the image\n",
    "    predicted_class : str\n",
    "        Predicted class display name (mapped)\n",
    "    confidence : float\n",
    "        Confidence score\n",
    "    probabilities : np.array\n",
    "        Probabilities for all classes\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"File: {os.path.basename(image_path)}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Predicted Class: {predicted_class}\")\n",
    "    print(f\"Confidence: {confidence:.2f}%\\n\")\n",
    "    \n",
    "    print(\"Class Probabilities:\")\n",
    "    print(f\"{'Class':<10} {'Probability':<15} {'Confidence %':<15}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for idx, raw_class in enumerate(le.classes_):\n",
    "        class_name = CLASS_DISPLAY_MAP.get(raw_class, raw_class)\n",
    "        prob = probabilities[idx]\n",
    "        percent = prob * 100\n",
    "        bar_length = int(percent / 5)\n",
    "        bar = \"█\" * bar_length + \"░\" * (20 - bar_length)\n",
    "        print(f\"{class_name:<10} {prob:<15.4f} {bar} {percent:6.2f}%\")\n",
    "    \n",
    "    print(f\"{'='*70}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb675900",
   "metadata": {},
   "source": [
    "## Usage Examples\n",
    "\n",
    "Use the functions to test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "53b96cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing JPG files in current directory...\n",
      "\n",
      "\n",
      "======================================================================\n",
      "TESTING 1 IMAGE(S)\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "File: IMG_1836.jpg\n",
      "======================================================================\n",
      "Predicted Class: ۆ\n",
      "Confidence: 99.92%\n",
      "\n",
      "Class Probabilities:\n",
      "Class      Probability     Confidence %   \n",
      "--------------------------------------------------\n",
      "ێ          0.0008          ░░░░░░░░░░░░░░░░░░░░   0.08%\n",
      "ڵ          0.0000          ░░░░░░░░░░░░░░░░░░░░   0.00%\n",
      "ۆ          0.9992          ███████████████████░  99.92%\n",
      "ڕ          0.0000          ░░░░░░░░░░░░░░░░░░░░   0.00%\n",
      "ڤ          0.0000          ░░░░░░░░░░░░░░░░░░░░   0.00%\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Test a single image\n",
    "# Uncomment and modify the path to test a specific image\n",
    "# image_path = \"path/to/your/image.jpg\"\n",
    "# predicted_class, confidence, probabilities = predict_single_image(image_path)\n",
    "# if predicted_class:\n",
    "#     display_prediction(image_path, predicted_class, confidence, probabilities)\n",
    "\n",
    "\n",
    "# Example 2: Test all JPG files in current directory\n",
    "print(\"Testing JPG files in current directory...\\n\")\n",
    "jpg_files = glob.glob(\"*.jpg\") + glob.glob(\"*.JPG\") + glob.glob(\"*.jpeg\") + glob.glob(\"*.JPEG\")\n",
    "\n",
    "if jpg_files:\n",
    "    test_multiple_images(file_list=sorted(jpg_files))\n",
    "else:\n",
    "    print(\"No JPG files found in current directory\")\n",
    "\n",
    "\n",
    "# Example 3: Test images in a specific folder\n",
    "# Uncomment to test images in a folder\n",
    "# test_multiple_images(folder_path=\"EEE_letters\")\n",
    "\n",
    "\n",
    "# Example 4: Test specific image files\n",
    "# Uncomment to test specific files\n",
    "# test_multiple_images(file_list=[\"image1.jpg\", \"image2.png\", \"image3.jpg\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4fa3d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3a93f426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv2 imported OK, version: 4.12.0\n"
     ]
    }
   ],
   "source": [
    "# Sanity check: ensure OpenCV (cv2) is available and show version\n",
    "import cv2\n",
    "print(\"cv2 imported OK, version:\", cv2.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c6f12ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch imported OK, version: 2.9.1\n",
      "CUDA available: False\n",
      "Using CPU\n"
     ]
    }
   ],
   "source": [
    "# Sanity check: ensure torch is available and show device\n",
    "import torch\n",
    "print(\"torch imported OK, version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU name:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6992b118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn imported OK, version: 1.7.2\n",
      "LabelEncoder available: True\n"
     ]
    }
   ],
   "source": [
    "# Sanity check: ensure scikit-learn is available and show version\n",
    "import sklearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "print(\"sklearn imported OK, version:\", sklearn.__version__)\n",
    "print(\"LabelEncoder available:\", LabelEncoder is not None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f28f11",
   "metadata": {},
   "source": [
    "## Live Camera GUI – What it does and how to use it\n",
    "\n",
    "This GUI provides a threaded live preview from your camera with a center ROI overlay. When you click Capture ROI, the selected area is converted to grayscale, binarized with Otsu thresholding, resized to 64×64, and classified by the CNN model.\n",
    "\n",
    "Controls:\n",
    "- Camera: choose a camera index (0/1/2)\n",
    "- Start: opens the camera and starts a smooth preview\n",
    "- Stop: stops preview and releases the camera\n",
    "- Capture ROI: classifies the current ROI and shows:\n",
    "  - ROI (left) and binarized copy (right)\n",
    "  - Predicted Kurdish letter (large)\n",
    "  - Confidence (%) and a per-class probability bar list\n",
    "\n",
    "Tips:\n",
    "- Place a black letter on white background inside the ROI\n",
    "- Ensure good lighting and focus\n",
    "- If you see no frames, try a different camera index or grant camera permissions\n",
    "\n",
    "Styling:\n",
    "- Only the predicted class glyph is large; labels and confidence remain normal size for readability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3f4e8d95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "945514b0c59d45f09753973c52afbcba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=\"\\n    <div style='font-family:Inter,Helvetica,Arial,sans-serif; padding:8px 0;'>\\n …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Interactive GUI: Camera ROI Capture and Prediction (robust, with diagnostics)\n",
    "\n",
    "# Enhancements:\n",
    "# - Camera index selector (0/1/2) and resolution hints\n",
    "# - Immediate single-frame test after start to confirm preview\n",
    "# - Heartbeat indicator updated from the preview loop\n",
    "# - Defensive checks and clear status messages\n",
    "# - Stop always releases camera; Start prevents duplicate threads\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import threading\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Helper: predict from a grayscale numpy array (already 2D)\n",
    "def predict_from_array(gray_array):\n",
    "    assert len(gray_array.shape) == 2, \"Expected grayscale 2D array\"\n",
    "    img_resized = cv2.resize(gray_array, (IMG_SIZE, IMG_SIZE))\n",
    "    img_normalized = img_resized / 255.0\n",
    "    img_tensor = torch.tensor(img_normalized, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(img_tensor)\n",
    "        probabilities = torch.softmax(output, dim=1)[0].cpu().numpy()\n",
    "        predicted_idx = int(torch.argmax(output, dim=1).item())\n",
    "        confidence = float(probabilities[predicted_idx] * 100)\n",
    "    raw_class = le.classes_[predicted_idx]\n",
    "    predicted_class_display = CLASS_DISPLAY_MAP.get(raw_class, raw_class)\n",
    "    return predicted_class_display, confidence, probabilities\n",
    "\n",
    "# Widgets\n",
    "cam_index_dropdown = widgets.Dropdown(options=[0,1,2], value=0, description='Camera', layout=widgets.Layout(width='180px'))\n",
    "start_button = widgets.Button(description=\"Start\", button_style=\"success\")\n",
    "stop_button = widgets.Button(description=\"Stop\", button_style=\"warning\")\n",
    "capture_button = widgets.Button(description=\"Capture ROI\", button_style=\"primary\")\n",
    "status_label = widgets.HTML(value=\"<b>Status:</b> Ready\")\n",
    "heartbeat_label = widgets.HTML(value=\"<b>Heartbeat:</b> idle\")\n",
    "output_area = widgets.Output()\n",
    "preview_area = widgets.VBox([])\n",
    "image_widget = widgets.Image(format='png')\n",
    "preview_area.children = [image_widget]\n",
    "\n",
    "controls = widgets.HBox([cam_index_dropdown, start_button, stop_button, capture_button])\n",
    "ui = widgets.VBox([\n",
    "    widgets.HTML(\"\"\"\n",
    "    <div style='font-family:Inter,Helvetica,Arial,sans-serif; padding:8px 0;'>\n",
    "      <h2 style='margin:0 0 8px;'>Kurdish Letter OCR – Live Capture</h2>\n",
    "      <p style='color:#555; margin:0;'>Place a white paper with a black Kurdish letter within the ROI, then press <b>Capture ROI</b>.</p>\n",
    "    </div>\n",
    "    \"\"\"),\n",
    "    controls,\n",
    "    status_label,\n",
    "    heartbeat_label,\n",
    "    preview_area,\n",
    "    output_area\n",
    "])\n",
    "\n",
    "display(ui)\n",
    "\n",
    "# Camera and ROI config\n",
    "cap = None\n",
    "running = False\n",
    "preview_thread = None\n",
    "last_frame = None\n",
    "roi = None  # (x,y,w,h)\n",
    "COLOR_ROI = (0,200,255)\n",
    "\n",
    "# Frame overlay\n",
    "def draw_overlay(frame):\n",
    "    h, w = frame.shape[:2]\n",
    "    global roi\n",
    "    if roi is None:\n",
    "        rw = int(w * 0.6); rh = int(h * 0.6)\n",
    "        rx = (w - rw) // 2; ry = (h - rh) // 2\n",
    "        roi = (rx, ry, rw, rh)\n",
    "    rx, ry, rw, rh = roi\n",
    "    overlay = frame.copy()\n",
    "    mask = np.zeros_like(frame)\n",
    "    cv2.rectangle(mask, (0,0), (w,h), (0,0,0), -1)\n",
    "    cv2.rectangle(mask, (rx, ry), (rx+rw, ry+rh), (0,0,0), -1)\n",
    "    alpha = 0.35\n",
    "    cv2.addWeighted(mask, alpha, overlay, 1-alpha, 0, overlay)\n",
    "    frame = overlay\n",
    "    cv2.rectangle(frame, (rx, ry), (rx+rw, ry+rh), COLOR_ROI, 2)\n",
    "    cv2.putText(frame, 'Align paper within ROI', (rx+10, max(30, ry-10)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, COLOR_ROI, 2, cv2.LINE_AA)\n",
    "    return frame\n",
    "\n",
    "# Binarize to white background / black text\n",
    "def to_binary(gray):\n",
    "    blur = cv2.GaussianBlur(gray, (5,5), 0)\n",
    "    _, thresh = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "    if np.mean(thresh) < 127:\n",
    "        thresh = 255 - thresh\n",
    "    return thresh\n",
    "\n",
    "# Preview loop\n",
    "\n",
    "def preview_loop():\n",
    "    global cap, running, last_frame\n",
    "    frame_count = 0\n",
    "    while running and cap is not None and cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            status_label.value = \"<b>Status:</b> Camera read failed\"\n",
    "            break\n",
    "        target_w = 800\n",
    "        target_h = int(frame.shape[0] * target_w / frame.shape[1])\n",
    "        frame = cv2.resize(frame, (target_w, target_h))\n",
    "        last_frame = frame.copy()\n",
    "        frame_disp = draw_overlay(frame.copy())\n",
    "        rgb = cv2.cvtColor(frame_disp, cv2.COLOR_BGR2RGB)\n",
    "        ok, buf = cv2.imencode('.png', rgb)\n",
    "        if ok:\n",
    "            image_widget.value = buf.tobytes()\n",
    "        frame_count += 1\n",
    "        if frame_count % 15 == 0:\n",
    "            heartbeat_label.value = f\"<b>Heartbeat:</b> {frame_count} frames\"\n",
    "        time.sleep(0.02)\n",
    "    heartbeat_label.value = \"<b>Heartbeat:</b> stopped\"\n",
    "\n",
    "# Event handlers\n",
    "\n",
    "def on_start_clicked(_):\n",
    "    global cap, running, preview_thread, last_frame\n",
    "    if running:\n",
    "        status_label.value = \"<b>Status:</b> Preview already running\"\n",
    "        return\n",
    "    cam_idx = cam_index_dropdown.value\n",
    "    cap = cv2.VideoCapture(cam_idx)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "    if not cap.isOpened():\n",
    "        status_label.value = f\"<b>Status:</b> Could not open camera index {cam_idx}\"\n",
    "        cap = None\n",
    "        return\n",
    "    ret, frame = cap.read()\n",
    "    if not ret or frame is None:\n",
    "        status_label.value = \"<b>Status:</b> Camera delivered no frames\"\n",
    "        cap.release(); cap = None\n",
    "        return\n",
    "    target_w = 800\n",
    "    target_h = int(frame.shape[0] * target_w / frame.shape[1])\n",
    "    frame = cv2.resize(frame, (target_w, target_h))\n",
    "    last_frame = frame.copy()\n",
    "    frame_disp = draw_overlay(frame.copy())\n",
    "    rgb = cv2.cvtColor(frame_disp, cv2.COLOR_BGR2RGB)\n",
    "    ok, buf = cv2.imencode('.png', rgb)\n",
    "    if ok:\n",
    "        image_widget.value = buf.tobytes()\n",
    "    status_label.value = \"<b>Status:</b> Camera started\"\n",
    "    heartbeat_label.value = \"<b>Heartbeat:</b> starting\"\n",
    "    running = True\n",
    "    preview_thread = threading.Thread(target=preview_loop, daemon=True)\n",
    "    preview_thread.start()\n",
    "\n",
    "\n",
    "def on_stop_clicked(_):\n",
    "    global cap, running, preview_thread\n",
    "    running = False\n",
    "    time.sleep(0.05)\n",
    "    if cap is not None:\n",
    "        cap.release()\n",
    "    cap = None\n",
    "    preview_thread = None\n",
    "    status_label.value = \"<b>Status:</b> Camera stopped\"\n",
    "\n",
    "\n",
    "def on_capture_clicked(_):\n",
    "    global last_frame, roi\n",
    "    if last_frame is None:\n",
    "        status_label.value = \"<b>Status:</b> No frame available; start camera first\"\n",
    "        return\n",
    "    rx, ry, rw, rh = roi if roi is not None else (0,0,last_frame.shape[1], last_frame.shape[0])\n",
    "    roi_img = last_frame[ry:ry+rh, rx:rx+rw]\n",
    "    if roi_img.size == 0:\n",
    "        status_label.value = \"<b>Status:</b> ROI out of bounds\"\n",
    "        return\n",
    "    gray = cv2.cvtColor(roi_img, cv2.COLOR_BGR2GRAY)\n",
    "    binary = to_binary(gray)\n",
    "\n",
    "    predicted_class_display, confidence, probabilities = predict_from_array(binary)\n",
    "\n",
    "    with output_area:\n",
    "        output_area.clear_output(wait=True)\n",
    "        roi_rgb = cv2.cvtColor(roi_img, cv2.COLOR_BGR2RGB)\n",
    "        bin_rgb = cv2.cvtColor(binary, cv2.COLOR_GRAY2RGB)\n",
    "        top = np.hstack([roi_rgb, bin_rgb])\n",
    "        caption = np.zeros((40, top.shape[1], 3), dtype=np.uint8)\n",
    "        cv2.putText(caption, 'ROI (left)  |  Binarized (right)', (10, 28), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255,255,255), 2, cv2.LINE_AA)\n",
    "        comp = np.vstack([caption, top])\n",
    "        ok, buf = cv2.imencode('.png', comp)\n",
    "        if ok:\n",
    "            display(widgets.Image(value=buf.tobytes(), format='png'))\n",
    "        prob_lines = []\n",
    "        for idx, raw_class in enumerate(le.classes_):\n",
    "            class_name = CLASS_DISPLAY_MAP.get(raw_class, raw_class)\n",
    "            percent = probabilities[idx] * 100\n",
    "            bar_len = int(percent / 5)\n",
    "            bar = '█' * bar_len + '░' * (20 - bar_len)\n",
    "            prob_lines.append(f\"{class_name}: {percent:6.2f}% {bar}\")\n",
    "        html = f\"\"\"\n",
    "        <div style='font-family:Inter,Helvetica,Arial,sans-serif; padding:8px 0;'>\n",
    "          <div style='margin:6px 0; line-height:1;'>\n",
    "            <span style='color:#0b5;'>Predicted:</span>\n",
    "            <b style='font-size:180px; line-height:1;'>{predicted_class_display}</b>\n",
    "            &nbsp;&nbsp;|&nbsp;&nbsp;\n",
    "            <span style='color:#06c;'>Confidence:</span>\n",
    "            <b>{confidence:.2f}%</b>\n",
    "          </div>\n",
    "          <pre style='background:#111; color:#ddd; padding:12px; border-radius:8px; line-height:1.4;'>\n",
    "        {\"\\n\".join(prob_lines)}\n",
    "          </pre>\n",
    "        </div>\n",
    "        \"\"\"\n",
    "        display(widgets.HTML(html))\n",
    "    status_label.value = \"<b>Status:</b> ROI captured and classified\"\n",
    "\n",
    "# Bind events\n",
    "start_button.on_click(on_start_clicked)\n",
    "stop_button.on_click(on_stop_clicked)\n",
    "capture_button.on_click(on_capture_clicked)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ad4cff8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57c16e3b2720418aafe3e0c4be79a90c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='success', description='Widgets OK', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ipywidgets imported OK\n"
     ]
    }
   ],
   "source": [
    "# Verify ipywidgets availability\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "btn = widgets.Button(description=\"Widgets OK\", button_style=\"success\")\n",
    "display(btn)\n",
    "print(\"ipywidgets imported OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e970030",
   "metadata": {},
   "source": [
    "## Troubleshooting & Tips\n",
    "\n",
    "Widgets don’t render:\n",
    "- Ensure the notebook is running in a Jupyter environment that supports ipywidgets\n",
    "- If necessary, install/enable ipywidgets in your environment and reload the notebook\n",
    "\n",
    "Camera doesn’t start or shows a black frame:\n",
    "- Try another Camera index from the dropdown (0, 1, 2)\n",
    "- Close other apps using the camera\n",
    "- On macOS, grant camera permission to your Jupyter app/kernel in System Settings → Privacy & Security → Camera\n",
    "\n",
    "Prediction seems incorrect or unstable:\n",
    "- Use high-contrast input (black ink on white paper)\n",
    "- Center the glyph fully within the ROI and reduce motion\n",
    "- Ensure the glyph resembles training data and isn’t mirrored/rotated\n",
    "\n",
    "Performance:\n",
    "- The model runs on CPU by default\n",
    "- If you have a compatible GPU, it will be used automatically when available\n",
    "\n",
    "Known assumptions:\n",
    "- Input is a single glyph centered in the ROI\n",
    "- Model expects 64×64 grayscale\n",
    "- Classes and UI display mapping:\n",
    "  - EEE_letters →ێ \n",
    "  - LLL_letters →ڵ \n",
    "  - OOO_letters →ۆ \n",
    "  - RRR_letters →ڕ \n",
    "  - VVV_letters →ڤ \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
