{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96237edc",
   "metadata": {},
   "source": [
    "# Smart Mirror Agent – Minimal, Stylish GUI with Camera Preview and Fashion Tips\n",
    "This notebook sets up a live webcam preview and uses the Qwen/Qwen3-VL-2B-Instruct vision-language model to provide tailored fashion tips based on the captured user image and a topic they provide."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa9ebc3",
   "metadata": {},
   "source": [
    "# 1) Install and Import Dependencies\n",
    "Below commands install required packages. If already installed, they will be skipped. Then we import needed modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b4a8b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install completed; import libraries\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import base64\n",
    "import threading\n",
    "from typing import Optional\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM\n",
    "\n",
    "# Configure M1 Max for maximum performance\n",
    "os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"  # Use all available GPU memory\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"  # Fallback for unsupported ops\n",
    "\n",
    "# Helper: ensure that processor/model exist even after kernel restarts\n",
    "processor = globals().get(\"processor\", None)\n",
    "model = globals().get(\"model\", None)\n",
    "\n",
    "# Device selection - prioritize MPS for M1 Max\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "device = globals().get(\"device\", device)\n",
    "\n",
    "# Use Qwen2.5-VL-3B-Instruct for facial feature recognition (VLM)\n",
    "selected_model_name = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
    "\n",
    "# Path to fine-tuned LoRA weights (set after training)\n",
    "LORA_WEIGHTS_PATH = globals().get(\"CHECKPOINT_PATH\", None)\n",
    "\n",
    "\n",
    "def ensure_model_ready():\n",
    "    global processor, model, device, selected_model_name, LORA_WEIGHTS_PATH\n",
    "    if processor is None or model is None:\n",
    "        # Use float32 on MPS for stability, bfloat16 on CUDA\n",
    "        if device == \"mps\":\n",
    "            dtype = torch.float32  # MPS works best with float32\n",
    "        elif device == \"cuda\":\n",
    "            dtype = torch.bfloat16\n",
    "        else:\n",
    "            dtype = torch.float32\n",
    "        \n",
    "        print(f\"Loading vision-language model: {selected_model_name} on {device}…\")\n",
    "        processor = AutoProcessor.from_pretrained(selected_model_name, trust_remote_code=True)\n",
    "        \n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            selected_model_name,\n",
    "            trust_remote_code=True,\n",
    "            torch_dtype=dtype,\n",
    "        )\n",
    "        \n",
    "        # Check if fine-tuned LoRA weights exist\n",
    "        if LORA_WEIGHTS_PATH and os.path.exists(LORA_WEIGHTS_PATH):\n",
    "            print(f\"Loading fine-tuned LoRA weights from {LORA_WEIGHTS_PATH}...\")\n",
    "            try:\n",
    "                from peft import PeftModel\n",
    "                model = PeftModel.from_pretrained(base_model, LORA_WEIGHTS_PATH)\n",
    "                print(\"✓ Fine-tuned model loaded!\")\n",
    "            except Exception as e:\n",
    "                print(f\"Could not load LoRA weights: {e}. Using base model.\")\n",
    "                model = base_model\n",
    "        else:\n",
    "            model = base_model\n",
    "            print(\"Using base model (no fine-tuned weights found).\")\n",
    "        \n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        \n",
    "        # Optimize for M1 Max\n",
    "        if device == \"mps\":\n",
    "            torch.mps.empty_cache()  # Clear any cached memory\n",
    "            print(\"✓ Model loaded on M1 Max GPU (MPS)\")\n",
    "        \n",
    "        print(\"Model ready.\")\n",
    "\n",
    "# Optional: explicit preload to avoid load on first capture\n",
    "def preload_model():\n",
    "    ensure_model_ready()\n",
    "    return \"Model ready.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b49b93c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Using M1 Max GPU (MPS) - Apple Silicon optimized\n",
      "  MPS available: True\n",
      "  MPS built: True\n",
      "\n",
      "Device set to: mps\n",
      "Model will be loaded on first analysis via ensure_model_ready()\n"
     ]
    }
   ],
   "source": [
    "# 2) Runtime Device Setup - Optimized for M1 Max\n",
    "# Prioritize MPS (Metal Performance Shaders) for Apple Silicon\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "    print(\"✓ Using M1 Max GPU (MPS) - Apple Silicon optimized\")\n",
    "    print(f\"  MPS available: {torch.backends.mps.is_available()}\")\n",
    "    print(f\"  MPS built: {torch.backends.mps.is_built()}\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    print(f\"Using CUDA GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"⚠ Using CPU (GPU not available)\")\n",
    "\n",
    "# Set high-performance mode for M1 Max\n",
    "if device == \"mps\":\n",
    "    # Enable async execution for better GPU utilization\n",
    "    import os\n",
    "    os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"  # Allow full GPU memory usage\n",
    "    \n",
    "print(f\"\\nDevice set to: {device}\")\n",
    "print(\"Model will be loaded on first analysis via ensure_model_ready()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73b6208",
   "metadata": {},
   "source": [
    "# 2A) Fine-tune Qwen2.5-VL-3B on CelebA Facial Attributes\n",
    "This section downloads CelebA-HQ, creates training pairs (image + attribute labels), and fine-tunes the VLM using **LoRA** (Low-Rank Adaptation) for efficient training. Training uses your **M1 Max GPU (MPS)** and stops after **15 minutes** with checkpoint saving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a25fa6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing compatible package versions...\n",
      "============================================================\n",
      "m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/3\u001b[0m [transformers]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/3\u001b[0m [transformers]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/3\u001b[0m [transformers]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/3\u001b[0m [transformers]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/3\u001b[0m [transformers]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/3\u001b[0m [transformers]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/3\u001b[0m [transformers]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/3\u001b[0m [transformers]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/3\u001b[0m [transformers]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/3\u001b[0m [transformers]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/3\u001b[0m [transformers]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/3\u001b[0m [transformers]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/3\u001b[0m [transformers]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/3\u001b[0m [transformers]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/3\u001b[0m [transformers]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/3\u001b[0m [transformers]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/3\u001b[0m [transformers]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m2/3\u001b[0m [peft]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [peft]\n",
      "\u001b[?25h\n",
      "\u001b[1A\u001b[2KSuccessfully installed accelerate-0.34.2 peft-0.14.0 transformers-4.47.0\n",
      "\n",
      "============================================================\n",
      "\n",
      "⚠️  KERNEL RESTART REQUIRED!\n",
      "Please restart the kernel now: Kernel → Restart Kernel\n",
      "Then run cells from Cell 1 again.\n",
      "============================================================\n",
      "\n",
      "Downloading CelebA-HQ dataset...\n",
      "Dataset downloaded to: /Users/ahmad/.cache/kagglehub/datasets/ipythonx/celebamaskhq/versions/1\n"
     ]
    }
   ],
   "source": [
    "# 2A-1) Install fine-tuning dependencies and download CelebA-HQ\n",
    "# ⚠️ IMPORTANT: After running this cell, you MUST restart the kernel!\n",
    "# Go to: Kernel -> Restart Kernel, then run cells from Cell 1 again.\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "print(\"Installing compatible package versions...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Uninstall first to clear any conflicting versions\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", \"transformers\", \"peft\", \"accelerate\"], \n",
    "               capture_output=True)\n",
    "\n",
    "# Install SPECIFIC compatible versions – tested combo that avoids optional backend import errors\n",
    "result = subprocess.run([\n",
    "    sys.executable, \"-m\", \"pip\", \"install\", \n",
    "    \"transformers==4.47.0\",  # Supports qwen2_5_vl and avoids is_flute_available import\n",
    "    \"peft==0.14.0\",          # Compatible with transformers 4.47.0\n",
    "    \"accelerate==0.34.2\",    # Stable with both\n",
    "    \"kagglehub\", \n",
    "    \"torchvision\",\n",
    "    \"qwen-vl-utils\"\n",
    "], capture_output=True, text=True)\n",
    "\n",
    "print(result.stdout[-2000:] if len(result.stdout) > 2000 else result.stdout)\n",
    "if result.returncode != 0:\n",
    "    print(\"STDERR:\", result.stderr[-1000:])\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n⚠️  KERNEL RESTART REQUIRED!\")\n",
    "print(\"Please restart the kernel now: Kernel → Restart Kernel\")\n",
    "print(\"Then run cells from Cell 1 again.\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Try to download CelebA dataset (this part doesn't need restart)\n",
    "try:\n",
    "    import kagglehub\n",
    "    print(\"\\nDownloading CelebA-HQ dataset...\")\n",
    "    celeba_path = kagglehub.dataset_download(\"ipythonx/celebamaskhq\")\n",
    "    print(f\"Dataset downloaded to: {celeba_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Dataset download will happen after restart: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2454c67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b094810c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking package versions (without importing modules)...\n",
      "==================================================\n",
      "  ✓ transformers: 4.47.0\n",
      "  ✓ peft: 0.14.0\n",
      "  ✓ accelerate: 0.34.2\n",
      "  ✓ torch: 2.9.1\n",
      "  ✓ torchvision: 0.24.1\n",
      "==================================================\n",
      "\n",
      "✓ All packages OK! Continue to next cell.\n",
      "Dataset path: /Users/ahmad/.cache/kagglehub/datasets/ipythonx/celebamaskhq/versions/1\n"
     ]
    }
   ],
   "source": [
    "# 2A-1b) Verify packages are correctly installed (run after kernel restart)\n",
    "# Use importlib.metadata to avoid importing heavy packages (which can trigger optional backends)\n",
    "from importlib.metadata import version, PackageNotFoundError\n",
    "\n",
    "\n",
    "def check_pkg_version(dist_name, expected=None):\n",
    "    try:\n",
    "        ver = version(dist_name)\n",
    "        status = \"✓\"\n",
    "        if expected and ver != expected:\n",
    "            status = f\"⚠️ (expected {expected}, got {ver})\"\n",
    "        print(f\"  {status} {dist_name}: {ver}\")\n",
    "        return expected is None or ver == expected\n",
    "    except PackageNotFoundError as e:\n",
    "        print(f\"  ✗ {dist_name}: MISSING - {e}\")\n",
    "        return False\n",
    "\n",
    "print(\"Checking package versions (without importing modules)...\")\n",
    "print(\"=\" * 50)\n",
    "ok = True\n",
    "ok &= check_pkg_version(\"transformers\", \"4.47.0\")\n",
    "ok &= check_pkg_version(\"peft\", \"0.14.0\")\n",
    "ok &= check_pkg_version(\"accelerate\", \"0.34.2\")\n",
    "ok &= check_pkg_version(\"torch\")\n",
    "ok &= check_pkg_version(\"torchvision\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if ok:\n",
    "    print(\"\\n✓ All packages OK! Continue to next cell.\")\n",
    "    \n",
    "    # Also set celeba_path if dataset was downloaded\n",
    "    import kagglehub\n",
    "    celeba_path = kagglehub.dataset_download(\"ipythonx/celebamaskhq\")\n",
    "    print(f\"Dataset path: {celeba_path}\")\n",
    "else:\n",
    "    print(\"\\n⚠️ Version mismatch detected!\")\n",
    "    print(\"Run the previous cell (Cell 6), restart kernel, then run from Cell 1.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c93b2ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking paths...\n",
      "  Images dir: /Users/ahmad/.cache/kagglehub/datasets/ipythonx/celebamaskhq/versions/1/CelebAMask-HQ/CelebA-HQ-img - exists: True\n",
      "  Attributes file: /Users/ahmad/.cache/kagglehub/datasets/ipythonx/celebamaskhq/versions/1/CelebAMask-HQ/CelebAMask-HQ-attribute-anno.txt - exists: True\n",
      "\n",
      "Loaded 30000 images with 40 attributes\n",
      "Sample attributes: ['5_o_Clock_Shadow', 'Arched_Eyebrows', 'Attractive', 'Bags_Under_Eyes', 'Bald', 'Bangs', 'Big_Lips', 'Big_Nose', 'Black_Hair', 'Blond_Hair']\n",
      "\n",
      "Sample descriptions:\n",
      "  0.jpg: a face with arched eyebrows, attractive appearance, bags under eyes, full lips, ...\n",
      "  1.jpg: a face with arched eyebrows, attractive appearance, blonde hair, heavy makeup, m...\n",
      "  2.jpg: a face with attractive appearance, bags under eyes, full lips, brown hair, high ...\n"
     ]
    }
   ],
   "source": [
    "# 2A-2) Parse CelebA attribute labels and create training dataset\n",
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image as PILImage\n",
    "\n",
    "# Paths inside downloaded dataset - CelebAMask-HQ subfolder structure\n",
    "CELEBA_ROOT = os.path.join(celeba_path, \"CelebAMask-HQ\")\n",
    "IMG_DIR = os.path.join(CELEBA_ROOT, \"CelebA-HQ-img\")\n",
    "ATTR_FILE = os.path.join(CELEBA_ROOT, \"CelebAMask-HQ-attribute-anno.txt\")\n",
    "\n",
    "# Verify paths exist\n",
    "print(f\"Checking paths...\")\n",
    "print(f\"  Images dir: {IMG_DIR} - exists: {os.path.exists(IMG_DIR)}\")\n",
    "print(f\"  Attributes file: {ATTR_FILE} - exists: {os.path.exists(ATTR_FILE)}\")\n",
    "\n",
    "# Read attribute labels\n",
    "with open(ATTR_FILE, \"r\") as f:\n",
    "    lines = f.read().strip().split(\"\\n\")\n",
    "num_images = int(lines[0])\n",
    "attr_names = lines[1].split()\n",
    "data_rows = [line.split() for line in lines[2:]]\n",
    "attr_df = pd.DataFrame(data_rows, columns=[\"filename\"] + attr_names)\n",
    "\n",
    "# Convert to int and map -1 to 0\n",
    "for col in attr_names:\n",
    "    attr_df[col] = attr_df[col].astype(int)\n",
    "attr_df[attr_names] = attr_df[attr_names].replace(-1, 0)\n",
    "\n",
    "print(f\"\\nLoaded {len(attr_df)} images with {len(attr_names)} attributes\")\n",
    "print(\"Sample attributes:\", attr_names[:10])\n",
    "\n",
    "# Human-readable attribute mapping\n",
    "ATTR_READABLE = {\n",
    "    \"5_o_Clock_Shadow\": \"5 o'clock shadow\",\n",
    "    \"Arched_Eyebrows\": \"arched eyebrows\",\n",
    "    \"Attractive\": \"attractive appearance\",\n",
    "    \"Bags_Under_Eyes\": \"bags under eyes\",\n",
    "    \"Bald\": \"bald head\",\n",
    "    \"Bangs\": \"bangs hairstyle\",\n",
    "    \"Big_Lips\": \"full lips\",\n",
    "    \"Big_Nose\": \"prominent nose\",\n",
    "    \"Black_Hair\": \"black hair\",\n",
    "    \"Blond_Hair\": \"blonde hair\",\n",
    "    \"Blurry\": \"blurry image\",\n",
    "    \"Brown_Hair\": \"brown hair\",\n",
    "    \"Bushy_Eyebrows\": \"bushy eyebrows\",\n",
    "    \"Chubby\": \"round face\",\n",
    "    \"Double_Chin\": \"double chin\",\n",
    "    \"Eyeglasses\": \"wearing eyeglasses\",\n",
    "    \"Goatee\": \"goatee beard\",\n",
    "    \"Gray_Hair\": \"gray hair\",\n",
    "    \"Heavy_Makeup\": \"heavy makeup\",\n",
    "    \"High_Cheekbones\": \"high cheekbones\",\n",
    "    \"Male\": \"masculine features\",\n",
    "    \"Mouth_Slightly_Open\": \"mouth slightly open\",\n",
    "    \"Mustache\": \"mustache\",\n",
    "    \"Narrow_Eyes\": \"narrow eyes\",\n",
    "    \"No_Beard\": \"clean shaven\",\n",
    "    \"Oval_Face\": \"oval face shape\",\n",
    "    \"Pale_Skin\": \"pale skin tone\",\n",
    "    \"Pointy_Nose\": \"pointy nose\",\n",
    "    \"Receding_Hairline\": \"receding hairline\",\n",
    "    \"Rosy_Cheeks\": \"rosy cheeks\",\n",
    "    \"Sideburns\": \"sideburns\",\n",
    "    \"Smiling\": \"smiling expression\",\n",
    "    \"Straight_Hair\": \"straight hair\",\n",
    "    \"Wavy_Hair\": \"wavy hair\",\n",
    "    \"Wearing_Earrings\": \"wearing earrings\",\n",
    "    \"Wearing_Hat\": \"wearing a hat\",\n",
    "    \"Wearing_Lipstick\": \"wearing lipstick\",\n",
    "    \"Wearing_Necklace\": \"wearing a necklace\",\n",
    "    \"Wearing_Necktie\": \"wearing a necktie\",\n",
    "    \"Young\": \"youthful appearance\",\n",
    "}\n",
    "\n",
    "def attrs_to_description(row):\n",
    "    \"\"\"Convert attribute row to natural language description.\"\"\"\n",
    "    present = [ATTR_READABLE.get(col, col.replace(\"_\", \" \").lower()) \n",
    "               for col in attr_names if row[col] == 1 and col not in [\"Blurry\"]]\n",
    "    if not present:\n",
    "        return \"a face with no distinctive features detected\"\n",
    "    return \"a face with \" + \", \".join(present[:12])  # limit for token length\n",
    "\n",
    "# Create training examples\n",
    "attr_df[\"description\"] = attr_df.apply(attrs_to_description, axis=1)\n",
    "print(\"\\nSample descriptions:\")\n",
    "for i in range(3):\n",
    "    print(f\"  {attr_df.iloc[i]['filename']}: {attr_df.iloc[i]['description'][:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c23644d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CelebAVLMDataset class defined. Dataset will be created after model loading.\n"
     ]
    }
   ],
   "source": [
    "# 2A-3) Create VLM fine-tuning dataset with image-text pairs\n",
    "import random\n",
    "\n",
    "class CelebAVLMDataset(Dataset):\n",
    "    \"\"\"Dataset for VLM fine-tuning: image + question -> attribute description answer.\"\"\"\n",
    "    \n",
    "    def __init__(self, df, img_dir, processor, max_samples=2000):\n",
    "        self.df = df.head(max_samples).reset_index(drop=True)\n",
    "        self.img_dir = img_dir\n",
    "        self.processor = processor\n",
    "        \n",
    "        # Different question variations for diversity\n",
    "        self.questions = [\n",
    "            \"Describe the facial features visible in this image.\",\n",
    "            \"What facial attributes can you observe in this person?\",\n",
    "            \"Analyze the face and list the visible features.\",\n",
    "            \"What do you notice about this person's facial features?\",\n",
    "            \"Describe the hair, face shape, and other visible attributes.\",\n",
    "        ]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = os.path.join(self.img_dir, row[\"filename\"])\n",
    "        \n",
    "        # Load and resize image\n",
    "        image = PILImage.open(img_path).convert(\"RGB\")\n",
    "        image = image.resize((384, 384))  # Resize for efficiency\n",
    "        \n",
    "        # Random question for variety\n",
    "        question = random.choice(self.questions)\n",
    "        answer = row[\"description\"]\n",
    "        \n",
    "        return {\n",
    "            \"image\": image,\n",
    "            \"question\": question,\n",
    "            \"answer\": answer,\n",
    "        }\n",
    "\n",
    "# We'll create the dataset after loading the model\n",
    "print(\"CelebAVLMDataset class defined. Dataset will be created after model loading.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8599d414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Using M1 Max GPU (MPS) for fine-tuning\n",
      "  MPS available: True\n",
      "  MPS built: True\n",
      "\n",
      "Loading processor for Qwen/Qwen2.5-VL-3B-Instruct...\n",
      "Loading Qwen/Qwen2.5-VL-3B-Instruct model...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The checkpoint you are trying to load has model type `qwen2_5_vl` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/Smart_Mirror_Finalized/.venv/lib/python3.13/site-packages/transformers/models/auto/configuration_auto.py:1038\u001b[39m, in \u001b[36mAutoConfig.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m   1037\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1038\u001b[39m     config_class = \u001b[43mCONFIG_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel_type\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m   1039\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/Smart_Mirror_Finalized/.venv/lib/python3.13/site-packages/transformers/models/auto/configuration_auto.py:740\u001b[39m, in \u001b[36m_LazyConfigMapping.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    739\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._mapping:\n\u001b[32m--> \u001b[39m\u001b[32m740\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[32m    741\u001b[39m value = \u001b[38;5;28mself\u001b[39m._mapping[key]\n",
      "\u001b[31mKeyError\u001b[39m: 'qwen2_5_vl'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 43\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODEL_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m model...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     41\u001b[39m dtype = torch.float32  \u001b[38;5;66;03m# MPS works best with float32\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m ft_model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m    \u001b[49m\u001b[43mMODEL_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Manual placement for MPS\u001b[39;49;00m\n\u001b[32m     48\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[38;5;66;03m# Configure LoRA - target the attention layers for efficient fine-tuning\u001b[39;00m\n\u001b[32m     51\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mConfiguring LoRA adapters...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/Smart_Mirror_Finalized/.venv/lib/python3.13/site-packages/transformers/models/auto/auto_factory.py:526\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    523\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mquantization_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    524\u001b[39m     _ = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mquantization_config\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m526\u001b[39m config, kwargs = \u001b[43mAutoConfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    527\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    528\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_unused_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    529\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    530\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    531\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    532\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    533\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    534\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    536\u001b[39m \u001b[38;5;66;03m# if torch_dtype=auto was passed here, ensure to pass it on\u001b[39;00m\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs_orig.get(\u001b[33m\"\u001b[39m\u001b[33mtorch_dtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/Smart_Mirror_Finalized/.venv/lib/python3.13/site-packages/transformers/models/auto/configuration_auto.py:1040\u001b[39m, in \u001b[36mAutoConfig.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m   1038\u001b[39m         config_class = CONFIG_MAPPING[config_dict[\u001b[33m\"\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m\"\u001b[39m]]\n\u001b[32m   1039\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1040\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1041\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe checkpoint you are trying to load has model type `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_dict[\u001b[33m'\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1042\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mbut Transformers does not recognize this architecture. This could be because of an \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1043\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33missue with the checkpoint, or because your version of Transformers is out of date.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1044\u001b[39m         )\n\u001b[32m   1045\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m config_class.from_dict(config_dict, **unused_kwargs)\n\u001b[32m   1046\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1047\u001b[39m     \u001b[38;5;66;03m# Fallback: use pattern matching on the string.\u001b[39;00m\n\u001b[32m   1048\u001b[39m     \u001b[38;5;66;03m# We go from longer names to shorter names to catch roberta before bert (for instance)\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: The checkpoint you are trying to load has model type `qwen2_5_vl` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date."
     ]
    }
   ],
   "source": [
    "# 2A-4) Load model and configure LoRA for efficient fine-tuning on M1 Max\n",
    "# Compatibility shim for transformers optional backend import changes\n",
    "import transformers\n",
    "import transformers.utils as _tf_utils\n",
    "# Older/alternate names used across versions (typo in some releases): is_soundfile_availble\n",
    "if not hasattr(_tf_utils, \"is_soundfile_availble\") and hasattr(_tf_utils, \"is_soundfile_available\"):\n",
    "    setattr(_tf_utils, \"is_soundfile_availble\", _tf_utils.is_soundfile_available)\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Configure M1 Max for maximum GPU utilization\n",
    "os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"  # Use all GPU memory\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"  # Fallback for unsupported ops\n",
    "\n",
    "# Use MPS (M1 Max GPU) for training - prioritize Apple Silicon\n",
    "if torch.backends.mps.is_available():\n",
    "    train_device = torch.device(\"mps\")\n",
    "    print(\"✓ Using M1 Max GPU (MPS) for fine-tuning\")\n",
    "    print(f\"  MPS available: {torch.backends.mps.is_available()}\")\n",
    "    print(f\"  MPS built: {torch.backends.mps.is_built()}\")\n",
    "elif torch.cuda.is_available():\n",
    "    train_device = torch.device(\"cuda\")\n",
    "    print(f\"✓ Using CUDA GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    train_device = torch.device(\"cpu\")\n",
    "    print(\"⚠ Using CPU for fine-tuning (will be slower)\")\n",
    "\n",
    "# Load processor - using Qwen2.5-VL-3B-Instruct (publicly available)\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
    "print(f\"\\nLoading processor for {MODEL_NAME}...\")\n",
    "ft_processor = AutoProcessor.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Load model - use float32 for MPS stability and performance\n",
    "print(f\"Loading {MODEL_NAME} model...\")\n",
    "dtype = torch.float32  # MPS works best with float32\n",
    "\n",
    "ft_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=dtype,\n",
    "    device_map=None,  # Manual placement for MPS\n",
    ")\n",
    "\n",
    "# Configure LoRA - target the attention layers for efficient fine-tuning\n",
    "print(\"Configuring LoRA adapters...\")\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # LoRA rank\n",
    "    lora_alpha=32,  # Scaling factor\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # Attention layers\n",
    ")\n",
    "\n",
    "# Apply LoRA to model\n",
    "ft_model = get_peft_model(ft_model, lora_config)\n",
    "ft_model.to(train_device)\n",
    "\n",
    "# Clear MPS cache for optimal memory usage\n",
    "if train_device.type == \"mps\":\n",
    "    torch.mps.empty_cache()\n",
    "    torch.mps.synchronize()\n",
    "    print(\"✓ MPS cache cleared for optimal memory\")\n",
    "\n",
    "# Print trainable parameters\n",
    "trainable_params = sum(p.numel() for p in ft_model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in ft_model.parameters())\n",
    "print(f\"\\nTrainable parameters: {trainable_params:,} / {total_params:,} ({100 * trainable_params / total_params:.2f}%)\")\n",
    "\n",
    "# Create dataset\n",
    "print(\"\\nCreating training dataset...\")\n",
    "train_dataset = CelebAVLMDataset(attr_df, IMG_DIR, ft_processor, max_samples=2000)\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"\\n✓ Ready to fine-tune on {train_device}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5b8044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2A-5) Fine-tune with 15-minute timeout, checkpointing, and M1 Max optimization\n",
    "import time as time_module\n",
    "from torch.optim import AdamW\n",
    "import gc\n",
    "\n",
    "# Training settings\n",
    "CHECKPOINT_DIR = os.path.join(celeba_path, \"qwen2_5_vl_finetuned\")\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "CHECKPOINT_PATH = os.path.join(CHECKPOINT_DIR, \"lora_weights\")\n",
    "MAX_TRAIN_SECONDS = 15 * 60  # 15 minutes\n",
    "\n",
    "# Optimizer with settings optimized for M1 Max\n",
    "optimizer = AdamW(ft_model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "\n",
    "# Training loop\n",
    "ft_model.train()\n",
    "start_time = time_module.time()\n",
    "global_step = 0\n",
    "best_loss = float(\"inf\")\n",
    "running_loss = 0.0\n",
    "log_interval = 10\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Starting fine-tuning on M1 Max (MPS)\")\n",
    "print(f\"Max training time: {MAX_TRAIN_SECONDS // 60} minutes\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Shuffle indices for random sampling\n",
    "indices = list(range(len(train_dataset)))\n",
    "random.shuffle(indices)\n",
    "\n",
    "for idx in indices:\n",
    "    # Check time limit\n",
    "    elapsed = time_module.time() - start_time\n",
    "    if elapsed >= MAX_TRAIN_SECONDS:\n",
    "        print(f\"\\n⏱ Time limit reached ({elapsed / 60:.1f} min). Stopping training.\")\n",
    "        break\n",
    "    \n",
    "    # Get sample\n",
    "    sample = train_dataset[idx]\n",
    "    image = sample[\"image\"]\n",
    "    question = sample[\"question\"]\n",
    "    answer = sample[\"answer\"]\n",
    "    \n",
    "    # Build conversation format for Qwen\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": image},\n",
    "                {\"type\": \"text\", \"text\": question},\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": answer\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        # Process inputs\n",
    "        text_prompt = ft_processor.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False\n",
    "        )\n",
    "        \n",
    "        inputs = ft_processor(\n",
    "            text=[text_prompt],\n",
    "            images=[image],\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "        )\n",
    "        \n",
    "        # Move to device\n",
    "        inputs = {k: v.to(train_device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = ft_model(**inputs, labels=inputs[\"input_ids\"])\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(ft_model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        global_step += 1\n",
    "        \n",
    "        # Log progress\n",
    "        if global_step % log_interval == 0:\n",
    "            avg_loss = running_loss / log_interval\n",
    "            elapsed_min = (time_module.time() - start_time) / 60\n",
    "            remaining_min = (MAX_TRAIN_SECONDS / 60) - elapsed_min\n",
    "            print(f\"Step {global_step}: loss={avg_loss:.4f}, elapsed={elapsed_min:.1f}min, remaining={remaining_min:.1f}min\")\n",
    "            \n",
    "            # Save checkpoint if improved\n",
    "            if avg_loss < best_loss:\n",
    "                best_loss = avg_loss\n",
    "                ft_model.save_pretrained(CHECKPOINT_PATH)\n",
    "                print(f\"  → Checkpoint saved (best_loss={best_loss:.4f})\")\n",
    "            \n",
    "            running_loss = 0.0\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error at step {global_step}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    # Periodic MPS memory management for M1 Max optimization\n",
    "    if global_step % 25 == 0:\n",
    "        gc.collect()\n",
    "        if train_device.type == \"mps\":\n",
    "            torch.mps.empty_cache()\n",
    "            torch.mps.synchronize()  # Ensure GPU operations complete\n",
    "\n",
    "# Final save\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"✓ Fine-tuning complete!\")\n",
    "ft_model.save_pretrained(CHECKPOINT_PATH)\n",
    "print(f\"Final checkpoint saved to: {CHECKPOINT_PATH}\")\n",
    "print(f\"Total steps: {global_step}\")\n",
    "print(f\"Best loss: {best_loss:.4f}\")\n",
    "print(f\"Total time: {(time_module.time() - start_time) / 60:.1f} minutes\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173ea4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2A-6) Load fine-tuned model for inference on M1 Max\n",
    "from peft import PeftModel\n",
    "import gc\n",
    "\n",
    "# Clean up training model to free memory\n",
    "del ft_model\n",
    "del optimizer\n",
    "gc.collect()\n",
    "\n",
    "# Clear MPS cache\n",
    "if torch.backends.mps.is_available():\n",
    "    torch.mps.empty_cache()\n",
    "    torch.mps.synchronize()\n",
    "\n",
    "print(\"Loading fine-tuned model for inference on M1 Max...\")\n",
    "\n",
    "# Use MPS for inference\n",
    "if torch.backends.mps.is_available():\n",
    "    inference_device = \"mps\"\n",
    "elif torch.cuda.is_available():\n",
    "    inference_device = \"cuda\"\n",
    "else:\n",
    "    inference_device = \"cpu\"\n",
    "\n",
    "inference_dtype = torch.float32  # MPS works best with float32\n",
    "\n",
    "# Use the same model name as training\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=inference_dtype,\n",
    ")\n",
    "\n",
    "# Load LoRA weights\n",
    "finetuned_model = PeftModel.from_pretrained(base_model, CHECKPOINT_PATH)\n",
    "finetuned_model.to(inference_device)\n",
    "finetuned_model.eval()\n",
    "\n",
    "# Clear cache after loading\n",
    "if inference_device == \"mps\":\n",
    "    torch.mps.empty_cache()\n",
    "    torch.mps.synchronize()\n",
    "\n",
    "# Update global model references for the GUI\n",
    "processor = ft_processor\n",
    "model = finetuned_model\n",
    "device = inference_device\n",
    "\n",
    "print(f\"✓ Fine-tuned model loaded on {inference_device}\")\n",
    "print(f\"✓ M1 Max GPU fully utilized for inference!\")\n",
    "print(f\"✓ Ready for facial feature analysis with improved accuracy!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26707911",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 09:03:41.226 Python[34872:1647488] WARNING: AVCaptureDeviceTypeExternal is deprecated for Continuity Cameras. Please use AVCaptureDeviceTypeContinuityCamera and add NSCameraUseContinuityCameraDeviceType to your Info.plist.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "894a6b07d8b9463386bfc0bace8e804f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'', layout=\"Layout(height='auto', width='100%')\")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9003c1720138404b9a943ff7b101ea8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value=\"<span style='color:#0a0'>Preview running</span>\")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 3) Initialize Camera and Live Preview\n",
    "import atexit\n",
    "\n",
    "# Gracefully stop any previous preview loop if re-running the cell\n",
    "try:\n",
    "    stop_preview()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "camera_index = 0  # default webcam\n",
    "cap = cv2.VideoCapture(camera_index)\n",
    "if not cap.isOpened():\n",
    "    raise RuntimeError(\"Failed to open webcam. Check permissions and camera availability.\")\n",
    "\n",
    "preview_running = False\n",
    "preview_thread: Optional[threading.Thread] = None\n",
    "current_frame_rgb: Optional[np.ndarray] = None\n",
    "\n",
    "# Widgets\n",
    "preview_image = widgets.Image(layout=widgets.Layout(width='100%', height='auto'))\n",
    "status_label = widgets.HTML(value=\"<span style='color:#888'>Preview stopped.</span>\")\n",
    "\n",
    "\n",
    "def bgr_to_png_bytes(frame_bgr: np.ndarray) -> bytes:\n",
    "    rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "    pil_img = Image.fromarray(rgb)\n",
    "    buf = io.BytesIO()\n",
    "    pil_img.save(buf, format='PNG')\n",
    "    return buf.getvalue()\n",
    "\n",
    "\n",
    "def preview_loop():\n",
    "    global preview_running, current_frame_rgb\n",
    "    try:\n",
    "        while preview_running:\n",
    "            ok, frame = cap.read()\n",
    "            if not ok:\n",
    "                time.sleep(0.03)\n",
    "                continue\n",
    "            # Convert to PNG for the Image widget\n",
    "            png_bytes = bgr_to_png_bytes(frame)\n",
    "            preview_image.value = png_bytes\n",
    "            current_frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            time.sleep(0.03)  # ~33 FPS cap to reduce CPU/GPU load\n",
    "    except Exception:\n",
    "        # Fail-safe: stop loop on unexpected errors to avoid kernel crash\n",
    "        preview_running = False\n",
    "\n",
    "\n",
    "def start_preview():\n",
    "    global preview_running, preview_thread\n",
    "    if preview_running:\n",
    "        return\n",
    "    preview_running = True\n",
    "    status_label.value = \"<span style='color:#0a0'>Preview running</span>\"\n",
    "    preview_thread = threading.Thread(target=preview_loop, daemon=True)\n",
    "    preview_thread.start()\n",
    "\n",
    "\n",
    "def stop_preview():\n",
    "    global preview_running, preview_thread\n",
    "    preview_running = False\n",
    "    status_label.value = \"<span style='color:#c00'>Preview stopped</span>\"\n",
    "    # Give the thread a moment to exit gracefully\n",
    "    time.sleep(0.1)\n",
    "\n",
    "\n",
    "# Ensure cleanup on kernel exit\n",
    "def _release_camera():\n",
    "    try:\n",
    "        cap.release()\n",
    "    except Exception:\n",
    "        pass\n",
    "atexit.register(_release_camera)\n",
    "\n",
    "start_preview()\n",
    "display(preview_image, status_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ac6bdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1156479a941c435bb0862af33fda2e0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='\\n<style>\\n  .smart-card {\\n    background:#fff; border:1px solid #eaeaea; border-r…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 4) Build Minimal GUI\n",
    "title = widgets.HTML(value=\"\"\"\n",
    "<div style='font-family:Inter,system-ui,Arial; font-weight:600; font-size:22px; color:#111; margin-bottom:8px;'>\n",
    "Smart Mirror – Image-Only Appearance Analysis</div>\n",
    "<div style='font-family:Inter,system-ui,Arial; font-size:13px; color:#555; margin-bottom:16px;'>\n",
    "Live preview below. Tap Capture for fast, image-only analysis.</div>\n",
    "\"\"\")\n",
    "\n",
    "# Capture button and output\n",
    "capture_btn = widgets.Button(description=\"Capture & Analyze\", button_style='',\n",
    "                              layout=widgets.Layout(width='100%'),\n",
    "                              tooltip=\"Capture current frame and generate image-only appearance analysis\")\n",
    "output_area = widgets.Output(layout=widgets.Layout(border='1px solid #eee', padding='10px'))\n",
    "\n",
    "style_html = widgets.HTML(value=\"\"\"\n",
    "<style>\n",
    "  .smart-card {\n",
    "    background:#fff; border:1px solid #eaeaea; border-radius:14px;\n",
    "    padding:16px; box-shadow:0 2px 10px rgba(0,0,0,0.06);\n",
    "  }\n",
    "  .row { display:flex; gap:16px; align-items:flex-start; }\n",
    "  .col { flex:1; }\n",
    "  .preview { border-radius:12px; overflow:hidden; border:1px solid #ddd; }\n",
    "  .label { font-family:Inter,system-ui,Arial; font-size:12px; color:#666; margin:6px 0 4px; }\n",
    "</style>\n",
    "\"\"\")\n",
    "\n",
    "ui = widgets.VBox([\n",
    "    style_html,\n",
    "    title,\n",
    "    widgets.VBox([\n",
    "        widgets.HTML(value=\"<div class='label'>Live Camera Preview</div>\"),\n",
    "        widgets.Box([preview_image], layout=widgets.Layout(css_classes=['preview'])),\n",
    "        status_label,\n",
    "    ], layout=widgets.Layout(css_classes=['smart-card'])),\n",
    "    widgets.VBox([\n",
    "        widgets.HTML(value=\"<div class='label'>Actions</div>\"),\n",
    "        capture_btn,\n",
    "    ], layout=widgets.Layout(css_classes=['smart-card'])),\n",
    "    widgets.VBox([\n",
    "        widgets.HTML(value=\"<div class='label'>Result</div>\"),\n",
    "        output_area,\n",
    "    ], layout=widgets.Layout(css_classes=['smart-card'])),\n",
    "], layout=widgets.Layout(width='800px'))\n",
    "\n",
    "display(ui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e218a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 & 6) Capture Frame, Tailored Prompt, and Run Inference\n",
    "\n",
    "# Comprehensive facial feature analysis from image only using Qwen2.5-VL-3B\n",
    "def compose_messages(image_pil: Image.Image):\n",
    "    # Encode image to base64 for processor consumption via 'image' content\n",
    "    buf = io.BytesIO()\n",
    "    image_pil.save(buf, format='PNG')\n",
    "    img_bytes = buf.getvalue()\n",
    "    img_b64 = base64.b64encode(img_bytes).decode('utf-8')\n",
    "\n",
    "    # System instruction: facial feature recognition and grooming tips\n",
    "    system_instruction = (\n",
    "        \"You are an expert facial feature analyst and grooming assistant. \"\n",
    "        \"Analyze the face in the image and describe ALL visible facial features in detail. \"\n",
    "        \"Include: face shape (oval, round, square, heart, oblong), forehead (high/low, wide/narrow), \"\n",
    "        \"eyebrows (shape, thickness, arch), eyes (shape, size, spacing), nose (shape, size, bridge), \"\n",
    "        \"cheekbones (high/low, prominent), lips (shape, fullness), jawline (defined/soft, angular/rounded), \"\n",
    "        \"chin (shape, prominence), skin texture and tone, facial hair if present (beard, mustache, stubble), \"\n",
    "        \"hair (length, texture, color, style, parting, hairline), and any accessories (glasses, earrings). \"\n",
    "        \"For any detail you cannot determine from the image, state 'cannot tell from image'. \"\n",
    "        \"Avoid sensitive attributes (age, health status, ethnicity). Be respectful and precise.\"\n",
    "    )\n",
    "\n",
    "    # Instruction: describe facial features first, then provide personalized tips\n",
    "    instruction = (\n",
    "        \"First, list 10-15 specific facial feature observations you can clearly see in the image. \"\n",
    "        \"Then provide 5-8 personalized grooming/styling tips based on those features \"\n",
    "        \"(e.g., hairstyles that complement face shape, beard styles, eyebrow grooming, skincare for skin type, \"\n",
    "        \"glasses frame suggestions if applicable). Use generic product types, not brands.\"\n",
    "    )\n",
    "\n",
    "    # Build chat messages with system + image + task request\n",
    "    content = [\n",
    "        {\"type\": \"text\", \"text\": system_instruction},\n",
    "        {\"type\": \"image\", \"image_base64\": img_b64, \"mime_type\": \"image/png\"},\n",
    "        {\"type\": \"text\", \"text\": (\n",
    "            \"Task: Analyze this face and describe all visible facial features in detail. \"\n",
    "            \"What do you observe about the face shape, forehead, eyebrows, eyes, nose, cheekbones, lips, jawline, chin, skin, facial hair, and hair? \"\n",
    "            \"Then provide personalized grooming and styling recommendations.\\n\\n\" + instruction\n",
    "        )},\n",
    "    ]\n",
    "    messages = [{\"role\": \"user\", \"content\": content}]\n",
    "    return messages\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_tips_from_frame(frame_rgb: np.ndarray) -> str:\n",
    "    ensure_model_ready()\n",
    "    pil_img = Image.fromarray(frame_rgb)\n",
    "    messages = compose_messages(pil_img)\n",
    "    inputs = processor.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=True,\n",
    "        return_dict=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    inputs = inputs.to(device)\n",
    "\n",
    "    # Generation settings optimized for detailed facial analysis\n",
    "    gen_kwargs = dict(\n",
    "        max_new_tokens=300,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.05,\n",
    "        no_repeat_ngram_size=3,\n",
    "    )\n",
    "    outputs = model.generate(**inputs, **gen_kwargs)\n",
    "\n",
    "    # Strip prompt tokens when decoding\n",
    "    prompt_len = inputs[\"input_ids\"].shape[-1]\n",
    "    text = processor.decode(outputs[0][prompt_len:])\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bdbb25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UI ready. Use the Capture & Analyze button.\n"
     ]
    }
   ],
   "source": [
    "# 7) Display Results in GUI – facial feature analysis via VLM\n",
    "\n",
    "def on_capture_clicked(btn):\n",
    "    with output_area:\n",
    "        clear_output()\n",
    "        try:\n",
    "            if current_frame_rgb is None:\n",
    "                print(\"No frame available. Try again.\")\n",
    "                return\n",
    "\n",
    "            # Show captured thumbnail\n",
    "            pil_img = Image.fromarray(current_frame_rgb)\n",
    "            thumb = pil_img.copy()\n",
    "            thumb.thumbnail((240, 240))\n",
    "            buf = io.BytesIO()\n",
    "            thumb.save(buf, format='PNG')\n",
    "            print(\"🔍 Analyzing your facial features…\")\n",
    "            display(Image.open(io.BytesIO(buf.getvalue())))\n",
    "\n",
    "            # Temporarily pause preview to free resources during generation\n",
    "            stop_preview()\n",
    "\n",
    "            # Run Qwen2.5-VL-3B for facial feature analysis and tips\n",
    "            analysis = generate_tips_from_frame(current_frame_rgb)\n",
    "            print(\"\\n🧠 Facial Feature Analysis & Tips:\")\n",
    "            print(analysis)\n",
    "        except RuntimeError as re:\n",
    "            print(\"RuntimeError during analysis:\", re)\n",
    "            print(\"Trying a safer re-run on CPU…\")\n",
    "            try:\n",
    "                global device\n",
    "                device = \"cpu\"\n",
    "                analysis = generate_tips_from_frame(current_frame_rgb)\n",
    "                print(analysis)\n",
    "            except Exception as e2:\n",
    "                print(\"Fallback failed:\", e2)\n",
    "        except Exception as e:\n",
    "            print(\"Error during analysis:\", e)\n",
    "        finally:\n",
    "            start_preview()\n",
    "\n",
    "capture_btn.on_click(on_capture_clicked)\n",
    "print(\"UI ready. Use the Capture & Analyze button.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4100cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "174900ef94944697a4fa3e0191fa19cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='warning', description='Stop Preview & Cleanup', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 8) Cleanup Resources on Kernel Stop\n",
    "def cleanup():\n",
    "    stop_preview()\n",
    "    try:\n",
    "        cap.release()\n",
    "    except Exception:\n",
    "        pass\n",
    "    print(\"Camera released.\")\n",
    " \n",
    "# Optional manual cleanup button\n",
    "cleanup_btn = widgets.Button(description=\"Stop Preview & Cleanup\", button_style='warning')\n",
    "def on_cleanup_clicked(btn):\n",
    "    cleanup()\n",
    "display(cleanup_btn)\n",
    "cleanup_btn.on_click(on_cleanup_clicked)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
